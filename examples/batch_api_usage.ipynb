{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae007755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from texttools.batch_manager import SimpleBatchManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34fafc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### best practice for connecting without error ####################\n",
    "\n",
    "# 1- using a proxy\n",
    "# 2- running the code on VPS\n",
    "\n",
    "# the first option is better, the data will be locally saved if anything went wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15654c58",
   "metadata": {},
   "source": [
    "# Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for batch ---\n",
    "class BatchConfig:\n",
    "    MAX_BATCH_SIZE = 1000  # Number of items per batch part\n",
    "    MAX_TOTAL_TOKENS = 2000000  # Max total tokens for all parts\n",
    "    CHARS_PER_TOKEN = 2.7\n",
    "    PROMPT_TOKEN_MULTIPLIER = 1000  # As in original code\n",
    "    BASE_OUTPUT_DIR = \"batch_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febfc84",
   "metadata": {},
   "source": [
    "# Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_batch(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Converts raw data to the required batch format: [{\"id\": int, \"content\": str}, ...]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for idx, item in enumerate(data):\n",
    "        if isinstance(item, dict) and \"content\" in item:\n",
    "            result.append({\"id\": item.get(\"id\", idx), \"content\": item[\"content\"]})\n",
    "        elif isinstance(item, str):\n",
    "            result.append({\"id\": idx, \"content\": item})\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid data item at index {idx}: {item}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc659901",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parsing_output(part_idx: int, output_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes the output from the batch manager. Here, just returns the data as-is.\n",
    "    Extend as needed for your use case.\n",
    "    \"\"\"\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38442e",
   "metadata": {},
   "source": [
    "# setup output structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should implement this however you want\n",
    "# the value for each key can be bool, integer, string or anything\n",
    "# the model will theoratically obey this structure\n",
    "\n",
    "# --- Output Model Example ---\n",
    "class OutputData(BaseModel):\n",
    "    desired_output: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019ac82",
   "metadata": {},
   "source": [
    "# setup BatchManager with BatchJobRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchJobRunner:\n",
    "    def __init__(self, \n",
    "                 system_prompt: str, \n",
    "                 job_name: str, \n",
    "                 input_data_path: str, \n",
    "                 output_data_path: str,\n",
    "                 model: str = \"gpt-4.1-mini\",\n",
    "                 output_model=OutputData):\n",
    "        self.config = BatchConfig()\n",
    "        self.system_prompt = system_prompt\n",
    "        self.job_name = job_name\n",
    "        self.input_data_path = input_data_path\n",
    "        self.output_data_path = output_data_path\n",
    "        self.model = model\n",
    "        self.output_model = output_model\n",
    "        self.manager = self._init_manager()\n",
    "        self.data: List[Dict[str, Any]] = []\n",
    "        self.parts: List[List[Dict[str, Any]]] = []\n",
    "        self._load_data()\n",
    "        self._partition_data()\n",
    "        Path(self.config.BASE_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _init_manager(self) -> SimpleBatchManager:\n",
    "        load_dotenv()\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        return SimpleBatchManager(\n",
    "            client=client,\n",
    "            model=self.model,\n",
    "            prompt_template=self.system_prompt,\n",
    "            output_model=self.output_model\n",
    "        )\n",
    "\n",
    "    def _load_data(self):\n",
    "        with open(self.input_data_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        self.data = data_for_batch(data)\n",
    "\n",
    "    def _partition_data(self):\n",
    "        total_length = sum(len(item[\"content\"]) for item in self.data)\n",
    "        prompt_length = len(self.system_prompt)\n",
    "        total = total_length + (prompt_length * len(self.data))\n",
    "        calculation = total / self.config.CHARS_PER_TOKEN\n",
    "        print(f\"Total chars: {total_length}, Prompt chars: {prompt_length}, Total: {total}, Tokens: {calculation}\")\n",
    "        if calculation < self.config.MAX_TOTAL_TOKENS:\n",
    "            self.parts = [self.data]\n",
    "        else:\n",
    "            # Partition into chunks of MAX_BATCH_SIZE\n",
    "            self.parts = [\n",
    "                self.data[i:i + self.config.MAX_BATCH_SIZE]\n",
    "                for i in range(0, len(self.data), self.config.MAX_BATCH_SIZE)\n",
    "            ]\n",
    "        print(f\"Data split into {len(self.parts)} part(s)\")\n",
    "\n",
    "    def run(self):\n",
    "        for idx, part in enumerate(self.parts):\n",
    "            part_job_name = f\"{self.job_name}_part_{idx+1}\" if len(self.parts) > 1 else self.job_name\n",
    "            print(f\"\\n--- Processing part {idx+1}/{len(self.parts)}: {part_job_name} ---\")\n",
    "            self._process_part(part, part_job_name, idx)\n",
    "\n",
    "    def _process_part(self, part: List[Dict[str, Any]], part_job_name: str, part_idx: int):\n",
    "        while True:\n",
    "            command = input(\"Enter command (1.start, 2.check, 3.fetch): \").strip().lower()\n",
    "            if command in [\"1\", \"start\"]:\n",
    "                self.manager.start(part, job_name=part_job_name)\n",
    "                print(\"Started batch job.\")\n",
    "                time.sleep(1)\n",
    "            elif command in [\"2\", \"check\"]:\n",
    "                status = self.manager.check_status(job_name=part_job_name)\n",
    "                print(f\"Status: {status}\")\n",
    "                time.sleep(5)\n",
    "                if status == \"completed\":\n",
    "                    print(\"Job completed. You can now fetch results.\")\n",
    "                elif status == \"failed\":\n",
    "                    print(\"Job failed. Clearing state.\")\n",
    "                    self.manager._clear_state(part_job_name)\n",
    "            elif command in [\"3\", \"fetch\"]:\n",
    "                output_data, log = self.manager.fetch_results(job_name=part_job_name, save=True, remove_cache=False)\n",
    "                output_data = parsing_output(part_idx, output_data)\n",
    "                self._save_results(output_data, log, part_idx)\n",
    "                print(\"Fetched and saved results for this part.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid command. Please enter 1, 2, or 3.\")\n",
    "\n",
    "    def _save_results(self, output_data: List[Dict[str, Any]], log: List[Any], part_idx: int):\n",
    "        part_suffix = f\"_part_{part_idx+1}\" if len(self.parts) > 1 else \"\"\n",
    "        result_path = Path(self.config.BASE_OUTPUT_DIR) / f\"{Path(self.output_data_path).stem}{part_suffix}.json\"\n",
    "        with open(result_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "        if log:\n",
    "            log_path = Path(self.config.BASE_OUTPUT_DIR) / f\"{Path(self.output_data_path).stem}{part_suffix}_log.json\"\n",
    "            with open(log_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(log, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a555ff",
   "metadata": {},
   "source": [
    "# start the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd899f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=== Batch Job Runner ===\")\n",
    "    system_prompt = input(\"Enter system prompt: \").strip()\n",
    "    job_name = input(\"Enter job name: \").strip()\n",
    "    input_data_path = input(\"Enter input data path (JSON): \").strip()\n",
    "    output_data_path = input(\"Enter output data path (JSON): \").strip()\n",
    "    runner = BatchJobRunner(system_prompt, job_name, input_data_path, output_data_path)\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
